{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting paddlenlp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/77/364cd13f3488bc22297f5e07be2a1faf04f939844a3ea3fd84e3ab79489f/paddlenlp-2.1.1-py3-none-any.whl (735kB)\n",
      "\u001b[K     |████████████████████████████████| 737kB 21kB/s eta 0:00:013\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Collecting paddlefsl==1.0.0 (from paddlenlp)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/65/9970dd09309eb673303206befc9f2fdc9c2d29d31f002ae8d6c7b442f562/paddlefsl-1.0.0-py3-none-any.whl (55kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 31kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)\n",
      "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.22.1)\n",
      "Collecting pillow==8.2.0 (from paddlefsl==1.0.0->paddlenlp)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/34/542152297dcc6c47a9dcb0685eac6d652d878ed3cea83bf2b23cb988e857/Pillow-8.2.0-cp37-cp37m-manylinux1_x86_64.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 41kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests~=2.24.0 (from paddlefsl==1.0.0->paddlenlp)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl (61kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 44kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm~=4.27.0 (from paddlefsl==1.0.0->paddlenlp)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/33/6d8bd6a7c4238f383426b7593bf05bfd6d9e1f10c3084b56c0f14d973754/tqdm-4.27.0-py2.py3-none-any.whl (44kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 44kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests~=2.24.0->paddlefsl==1.0.0->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests~=2.24.0->paddlefsl==1.0.0->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests~=2.24.0->paddlefsl==1.0.0->paddlenlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests~=2.24.0->paddlefsl==1.0.0->paddlenlp) (2019.9.11)\n",
      "\u001b[31mERROR: paddlefsl 1.0.0 has requirement numpy~=1.19.2, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pillow, requests, tqdm, paddlefsl, paddlenlp\n",
      "  Found existing installation: Pillow 7.1.2\n",
      "    Uninstalling Pillow-7.1.2:\n",
      "      Successfully uninstalled Pillow-7.1.2\n",
      "  Found existing installation: requests 2.22.0\n",
      "    Uninstalling requests-2.22.0:\n",
      "      Successfully uninstalled requests-2.22.0\n",
      "  Found existing installation: tqdm 4.36.1\n",
      "    Uninstalling tqdm-4.36.1:\n",
      "      Successfully uninstalled tqdm-4.36.1\n",
      "  Found existing installation: paddlenlp 2.0.1\n",
      "    Uninstalling paddlenlp-2.0.1:\n",
      "      Successfully uninstalled paddlenlp-2.0.1\n",
      "Successfully installed paddlefsl-1.0.0 paddlenlp-2.1.1 pillow-8.2.0 requests-2.24.0 tqdm-4.27.0\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a4/6d/6463d49a933f547439d6b5b98b46af8742cc03ae83543e4d7688c2420f8b/pip-21.3.1-py3-none-any.whl (1.7MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7MB 5.3MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 19.2.3\n",
      "    Uninstalling pip-19.2.3:\n",
      "      Successfully uninstalled pip-19.2.3\n",
      "Successfully installed pip-21.3.1\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (0.1.85)\n",
      "Collecting sentencepiece\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "     |████████████████████████████████| 1.2 MB 4.1 MB/s            \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.1.85\n",
      "    Uninstalling sentencepiece-0.1.85:\n",
      "      Successfully uninstalled sentencepiece-0.1.85\n",
      "Successfully installed sentencepiece-0.1.96\n"
     ]
    }
   ],
   "source": [
    "#配置环境\n",
    "!pip install --upgrade paddlenlp -i https://pypi.org/simple\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-11-24 19:54:01,681] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/unified_transformer/plato-mini-vocab.txt and saved to /home/aistudio/.paddlenlp/models/plato-mini\n",
      "[2021-11-24 19:54:01,684] [    INFO] - Downloading plato-mini-vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/unified_transformer/plato-mini-vocab.txt\n",
      "100%|██████████| 410/410 [00:00<00:00, 2881.56it/s]\n",
      "[2021-11-24 19:54:01,946] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/unified_transformer/plato-mini-spm.model and saved to /home/aistudio/.paddlenlp/models/plato-mini\n",
      "[2021-11-24 19:54:01,949] [    INFO] - Downloading plato-mini-spm.model from https://paddlenlp.bj.bcebos.com/models/transformers/unified_transformer/plato-mini-spm.model\n",
      "100%|██████████| 712/712 [00:00<00:00, 3230.41it/s]\n",
      "[2021-11-24 19:54:02,365] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/unified_transformer/plato-mini.pdparams and saved to /home/aistudio/.paddlenlp/models/plato-mini\n",
      "[2021-11-24 19:54:02,368] [    INFO] - Downloading plato-mini.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/unified_transformer/plato-mini.pdparams\n",
      "100%|██████████| 530157/530157 [00:07<00:00, 68727.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import UnifiedTransformerTokenizer\r\n",
    "\r\n",
    "# 设置想要使用模型的名称\r\n",
    "model_name = 'plato-mini'\r\n",
    "tokenizer = UnifiedTransformerTokenizer.from_pretrained(model_name)\r\n",
    "from paddlenlp.transformers import UnifiedTransformerLMHeadModel\r\n",
    "\r\n",
    "model = UnifiedTransformerLMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\r\n",
    "from functools import partial\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import paddle\r\n",
    "import paddle.distributed as dist\r\n",
    "from paddle.io import DataLoader, DistributedBatchSampler, BatchSampler\r\n",
    "from paddlenlp.data import Pad\r\n",
    "\r\n",
    "\r\n",
    "def print_args(args):\r\n",
    "    print('-----------  Configuration Arguments -----------')\r\n",
    "    for arg, value in sorted(vars(args).items()):\r\n",
    "        print('%s: %s' % (arg, value))\r\n",
    "    print('------------------------------------------------')\r\n",
    "\r\n",
    "def set_seed(seed):\r\n",
    "    # Use the same data seed(for data shuffle) for all procs to guarantee data\r\n",
    "    # consistency after sharding.\r\n",
    "    random.seed(seed)\r\n",
    "    np.random.seed(seed)\r\n",
    "    # Maybe different op seeds(for dropout) for different procs is better.\r\n",
    "    paddle.seed(seed + dist.get_rank())\r\n",
    "\r\n",
    "def post_process_response(token_ids, tokenizer):\r\n",
    "    \"\"\"Post-process the decoded sequence. Truncate from the first <eos>.\"\"\"\r\n",
    "    eos_pos = len(token_ids)\r\n",
    "    for i, tok_id in enumerate(token_ids):\r\n",
    "        if tok_id == tokenizer.sep_token_id:\r\n",
    "            eos_pos = i\r\n",
    "            break\r\n",
    "    token_ids = token_ids[:eos_pos]\r\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\r\n",
    "    tokens = tokenizer.merge_subword(tokens)\r\n",
    "    return token_ids, tokens\r\n",
    "\r\n",
    "def get_in_turn_repetition(pred, is_cn=False):\r\n",
    "    \"\"\"Get in-turn repetition.\"\"\"\r\n",
    "    if len(pred) == 0:\r\n",
    "        return 1.0\r\n",
    "    if isinstance(pred[0], str):\r\n",
    "        pred = [tok.lower() for tok in pred]\r\n",
    "        if is_cn:\r\n",
    "            pred = \"\".join(pred)\r\n",
    "    tri_grams = set()\r\n",
    "    for i in range(len(pred) - 2):\r\n",
    "        tri_gram = tuple(pred[i:i + 3])\r\n",
    "        if tri_gram in tri_grams:\r\n",
    "            return True\r\n",
    "        tri_grams.add(tri_gram)\r\n",
    "    return False\r\n",
    "\r\n",
    "\r\n",
    "def select_response(ids,\r\n",
    "                    scores,\r\n",
    "                    tokenizer,\r\n",
    "                    max_dec_len=None,\r\n",
    "                    num_return_sequences=1,\r\n",
    "                    keep_space=True):\r\n",
    "    ids = ids.numpy().tolist()\r\n",
    "    scores = scores.numpy()\r\n",
    "\r\n",
    "    if len(ids) != len(scores) or (len(ids) % num_return_sequences) != 0:\r\n",
    "        raise ValueError(\r\n",
    "            \"the length of `ids` is {}, but the `num_return_sequences` is {}\".\r\n",
    "            format(len(ids), num_return_sequences))\r\n",
    "\r\n",
    "    group = []\r\n",
    "    tmp = []\r\n",
    "    for pred, score in zip(ids, scores):\r\n",
    "        pred_token_ids, pred_tokens = post_process_response(pred, tokenizer)\r\n",
    "        num_token = len(pred_token_ids)\r\n",
    "        if keep_space:\r\n",
    "            response = \" \".join(pred_tokens)\r\n",
    "        else:\r\n",
    "            response = \"\".join(pred_tokens)\r\n",
    "\r\n",
    "        in_turn_repetition = get_in_turn_repetition(\r\n",
    "            pred_tokens, True) or get_in_turn_repetition(pred_token_ids)\r\n",
    "        # not ending\r\n",
    "        if max_dec_len is not None and num_token >= max_dec_len:\r\n",
    "            score -= 1e3\r\n",
    "        elif in_turn_repetition:\r\n",
    "            score -= 1e3\r\n",
    "\r\n",
    "        tmp.append([response, score])\r\n",
    "        if len(tmp) == num_return_sequences:\r\n",
    "            group.append(tmp)\r\n",
    "            tmp = []\r\n",
    "\r\n",
    "    results = []\r\n",
    "    for preds in group:\r\n",
    "        preds = sorted(preds, key=lambda x: -x[1])\r\n",
    "        results.append(preds[0][0])\r\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fun():\r\n",
    "    str_question=input()\r\n",
    "    print(\"Question:\"+str_question)\r\n",
    "    user_input=[]\r\n",
    "    user_input.append(str_question)\r\n",
    "    # 调用dialogue_encode方法生成输入\r\n",
    "    encoded_input = tokenizer.dialogue_encode(\r\n",
    "        user_input,\r\n",
    "        add_start_token_as_response=True,\r\n",
    "        return_tensors=True,\r\n",
    "        is_split_into_words=False\r\n",
    "    )\r\n",
    "    ids, scores = model.generate(\r\n",
    "        input_ids=encoded_input['input_ids'],\r\n",
    "        token_type_ids=encoded_input['token_type_ids'],\r\n",
    "        position_ids=encoded_input['position_ids'],\r\n",
    "        attention_mask=encoded_input['attention_mask'],\r\n",
    "        max_length=64,\r\n",
    "        min_length=1,\r\n",
    "        decode_strategy='sampling',\r\n",
    "        top_k=5,\r\n",
    "        num_return_sequences=20\r\n",
    "    )\r\n",
    "    #简单根据概率选取最佳回复\r\n",
    "    result = select_response(ids, scores, tokenizer, keep_space=False, num_return_sequences=20)\r\n",
    "    print(\"Answer:\"+result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:你好\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.784 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:你好,很高兴认识你\n",
      "Question:你是做什么的啊\n",
      "Answer:我在一家互联网公司上班,你是做什么的啊?\n"
     ]
    }
   ],
   "source": [
    "while(True):\r\n",
    "    fun()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
